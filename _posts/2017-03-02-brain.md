---
layout: post
permalink: /brain/
title: machine learning + visualization + connectomics
---

![Rainbow Brain]({{ site.baseurl }}/gfx/rainbow_brain.png "Rainbow Brain"){: .frontimg} ![Chip Brain]({{ site.baseurl }}/gfx/chip_brain.png "Chip Brain"){: .frontimg}

In connectomics we create a map of the mammalian brain at nano-scale. For this one must acquire image stacks of the brain of a rodent using electron microscopy. These images are of such high resolution that individual neurons (nerve cells) and their connections (synapses) are then visible. <!-- more --> Once these images are acquired, machine learning algorithms then classify cell structures and connections within these massive (terabytes or petabytes) images as manual processing is, thus far not possible.

![Neurons in Rat Brain]({{ site.baseurl }}/gfx/cells.jpg "Neurons in Rat Brain")
<small><br>A slice of rat brain</small>

<video autoplay loop="loop" style="object-fit:fill; float:right; margin-right:14px;max-width:100%;" class="frontimg" title="Neurons in 3D"><source src="{{ site.baseurl }}/gfx/3dconnectomics.1.mp4" type="video/mp4"></video>
Understanding the wiring of the brain, curing mental and neurological diseases, and deriving new artificial intelligence methods are ultimate goals. While these goals are far from achieved, the next developmental milestones are a fully processed 100 micron cube followed by itsâ€™ extension to a 1 millimeter cube of brain tissue. The video on the right shows selected neurons in a 100 micron cube.

<small>
[1] A. Suissa-Peleg, D. Haehn, S. Knowles-Barley, V. Kaynig, T.R. Jones, A. Wilson, R. Schalek, J.W. Lichtman, H. Pfister: [Automatic Neural Reconstruction from Petavoxel of Electron Microscopy Data](https://www.cambridge.org/core/journals/microscopy-and-microanalysis/article/div-classtitleautomatic-neural-reconstruction-from-petavoxel-of-electron-microscopy-datadiv/44219CFD1C9F1DE998BF8746F9F4C703), Microscopy and Microanalysis, 2016.<br>
[2] R. Schalek, D. Lee, N. Kasthuri, A. Suissa-Peleg, T.R. Jones, V. Kaynig, D. Haehn, H. Pfister, D. Cox, J.W. Lichtman: [Imaging a 1 mm 3 Volume of Rat Cortex Using a MultiBeam SEM](https://www.cambridge.org/core/journals/microscopy-and-microanalysis/article/div-classtitlespan-classboldimaging-a-1-mmspanspan-classsupspan-classbold3spanspanspan-classboldvolume-of-rat-cortex-using-a-multibeam-semspandiv/CF83F558CF849A0E64A09BDA4EA74550), Microscopy and Microanalysis, 2016.
</small>

## proofreading

Currently, the automatic classification of cells and connections is far from absolute. Humans are still needed to double-check the results and this task is called proofreading. In 2014 we published [Dojo](http://rhoana.org/dojo/), which is an interactive proofreading software that enables proofreading for untrained individuals recruited from all walks of life.

![Interactive Proofreading using Dojo]({{ site.baseurl }}/gfx/dojo.jpg "Interactive Proofreading using Dojo")
<small><br>The Dojo proofreading software to fix automatic labelings</small>

All of the results and data that have been gathered from the published user study can be found in [The Proofreading Benchmark](https://github.com/haehn/proofreading).

<video autoplay loop="loop" style="object-fit:fill; float:left; margin-right:14px;" class="frontimg" title="Guided Proofreading"><source src="{{ site.baseurl }}/gfx/guidedproofreading_small.mp4" type="video/mp4"></video> We found that the majority of time during interactive proofreading is spent looking for errors. 

We discovered that the majority of time spent during interactive proofreading is looking for errors. To reduce this, we developed the [Guided Proofreading](https://github.com/VCG/guidedproofreading/) system. The artificial intelligence within our system suggests potential errors and corrections to a user thus speeding up the proofreading task. Our results have demonstrated our trained classifier is also able to perform proofreading automatically up to a certain threshold. For increased performance, the Guided Proofreading user interface simplifies error correction by humans to a yes/no decision (see video on the left).

<small>
[3] D. Haehn, S. Knowles-Barley, M. Roberts, J. Beyer, N. Kasthuri, J.W. Lichtman, H. Pfister: [Design and evaluation of interactive proofreading tools for connectomics](http://ieeexplore.ieee.org/abstract/document/6875931/), IEEE transactions on visualization and computer graphics, 2014.<br>
[4] D. Haehn, V. Kaynig, J. Tompkin, J.W. Lichtman, H. Pfister: [Guided Proofreading of Automatic Segmentations for Connectomics](https://arxiv.org/abs/1704.00848), CoRR, 2017.
</small>

## visualization

Brain data is beautiful and not only at a nano-scale resolution. In 2012 we developed [XTK](http://goXTK.com), the first web-based visualization framework for medical imaging data such as MRI scans. This framework quickly became an established collaborative effort with over twenty international contributors which powers many projects world-wide.

![My Brain visualized using XTK]({{ site.baseurl }}/gfx/mybrain.jpg "My Brain visualized using XTK")
<small><br>A glimpse into my brain rendered from an MRI scan using XTK</small>

One example of these projects is our award-winning software, [Slice:Drop](http://slicedrop.com), a web-based viewer that supports many different medical imaging formats. Not only clinicians but researchers, as well as patients, now use this viewer daily. The software visualizes data without requiring any server uploads and leverages the local graphics card for accelerated rendering.

![The Slice:Drop viewer]({{ site.baseurl }}/gfx/slicedrop2.png "The Slice:Drop viewer")
<small><br>The Slice:Drop viewer supports different visualizations</small>

In turn, Connectomics data is much larger in size thus making visualization more difficult. We have successfully applied new processing strategies to overcome these challenges and have developed the [MBeam viewer](http://github.com/rhoana/mb) for the [ZEISS MultiSEM 505 microscope](https://www.zeiss.com/microscopy/int/products/scanning-electron-microscopes/multisem.html). This software allows neuroscientists to view the high resolution images immediately after acquisition. This process has been so successful that Zeiss, the manufacturer of the microscope, installed the MBeam viewer internally.

![The ZEISS MultiSEM 505 microscope with Jeff Lichtman in front of it]({{ site.baseurl }}/gfx/multisem.jpg "The ZEISS MultiSEM 505 microscope with Jeff Lichtman in front of it") ![The MBeam viewer]({{ site.baseurl }}/gfx/mbeam.png "The MBeam viewer")
<small><br>The ZEISS MultiSEM 505 microscope (with [Jeff Lichtman](http://lichtmanlab.fas.harvard.edu/) in front of it) and the MBeam viewer</small>

From an engineering standpoint, the MBeam viewer along with Dojo, our proofreading software, provide overlapping functionality such as the processing of data for transfer and visualization. 

![Butterfly]({{ site.baseurl }}/gfx/bfly.png "Butterfly"){: .center-image} 

To avoid duplication, we have now developed [Butterfly](https://github.com/rhoana/butterfly/), a system for scalable data management which unifies logic and visualization. As a result we can now create visualizations of large scientific datasets for every step of the connectomics processing pipeline.

![The Connectomics workflow]({{ site.baseurl }}/gfx/pipeline.png "The Connectomics workflow")
<small><br>Our Butterfly application suite includes specifically targeted visualizations for every step of the connectomics workflow and is fully extendible for
future visualizations</small>

In addition, we have recently added the Neural Data Queries interface to the Butterfly platform, allowing semantic queries of neurons and their synaptic connections. This functionality is a core component in evaluating the performance of our connectomics pipeline for the Machine Intelligence from Cortical Networks (MICroNS) effort, a $100 million research program backed by the Intelligence Advanced Research Projects Activity (IARPA).

<small>
[5] D. Haehn, N. Rannou, B. Ahtam, E. Grant, R. Pienaar: [Neuroimaging in the browser using the X Toolkit](http://www.frontiersin.org/10.3389/conf.fninf.2014.08.00101/event_abstract), Frontiers in Neuroinformatics, 2014.<br>
[6] D. Haehn, N. Rannou, E. Grant, R. Pienaar: [Slice: drop: Collaborative medical imaging in the browser](http://dl.acm.org/citation.cfm?id=2503645), ACM SIGGRAPH Computer Animation Festival, 2013.<br>
[7] D. Haehn, J. Hoffer, B. Matejek, A. Suissa-Peleg, A.K. Al-Awami, L. Kamentsky, F. Gonda, E. Meng, W. Zhang, R. Schalek, A. Wilson, T. Parag, J. Beyer, V. Kaynig, T.R. Jones, J. Tompkin, M. Hadwiger, J.W. Lichtman, H. Pfister: [Scalable Interactive Visualization for Connectomics](http://dx.doi.org/10.3390/informatics4030029), Informatics, 2017.
</small>

## further reading

[Machine Intelligence from Cortical Networks (MICrONS)](https://www.iarpa.gov/index.php/research-programs/microns)

[Terascale Neuroscience](https://neurodata.io/)

[AMI.js: Medical Imaging JavaScript ToolKit](https://github.com/FNNDSC/ami#readme)

[A Transfer Function Editor for Browsers](http://afruehstueck.github.io/TF.html)

[Neuroglancer: a WebGL-based viewer for volumetric data](https://github.com/google/neuroglancer)

[Machine Learning for humans](https://ironman5366.github.io/learn-blog/)

[The Visual Computing Group @ Harvard](http://vcg.seas.harvard.edu/)
